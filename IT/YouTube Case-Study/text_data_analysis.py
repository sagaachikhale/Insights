# -*- coding: utf-8 -*-
"""Text_Data_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qclxkVft5wCvB0qE2tblMvpVmiImPq2x
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

path = (r'/content/drive/MyDrive/Python Project Practise/Youtube/UScomments.csv')
a = pd.read_csv(path, on_bad_lines='skip')
print(a)

a.info() #to check info of data

a.describe() #to check statistical info of data

a.isnull().sum() #to check null values

rows_null = a[a['comment_text'].isnull()] #giving rows with null values present in given column
print(rows_null)

a.dropna(inplace=True) #dropping null values
print(a)

a.info() #checking info after dropping null values

"""**Performing Sentiment Analysis**"""

from textblob import TextBlob

#Break text into pieces: It can chop up a sentence into individual words. (Tokenization)
#Find important words: It can pick out the key words in a sentence, like nouns and verbs. (Part-of-speech tagging)
#Understand feelings: It can tell if a text is happy, sad, or neutral. (Sentiment analysis)
#Fix typos: It can help correct spelling mistakes. (Spelling correction)
#Find similar words: It can find words that mean almost the same thing. (Lemmatization)
#Translate languages: It can change text from one language to another. (Translation)
#Find patterns: It can look for repeated words or phrases. (N-grams)

sentiment1 = []
for i in a['comment_text']:
  try:
    sentiment1.append(TextBlob(i).sentiment.polarity)
  except:
    sentiment1.append(0)

a['polarity'] = sentiment1
print(a)

"""

```
# from textblob import TextBlob

def calculate_polarity(text):
  try:
    return TextBlob(text).sentiment.polarity
  except Exception as e:
    print(f"Error processing text: {e}")
    return 0  # Or handle the error differently

a['polarity'] = a['comment_text'].apply(calculate_polarity)
```

"""

a #called to check new column called 'polarity'

#creating new column called sentiment
a.loc[a['polarity'] == -1, 'sentiment'] = 'negative'
a.loc[a['polarity'] == 0, 'sentiment'] = 'neutral'
a.loc[a['polarity'] == 1, 'sentiment'] = 'positive'
# Handle potential NaN values
#a['sentiment'].fillna('neutral', inplace=True)
print(a)
#

a['sentiment'].unique()

negative_polarity = a[a['polarity'] == -1]
print(negative_polarity)
positive_polarity = a[a['polarity'] == 1]
print(positive_polarity)
#

"""**Performing wordclod analyis on positive and negative words**"""

from wordcloud import WordCloud, STOPWORDS

set(STOPWORDS)

a['comment_text']
negative_word = ' '.join(negative_polarity['comment_text'])
print(negative_word)
positive_word = ' '.join(positive_polarity['comment_text'])
print(positive_word)
#

wordcloud_negative = WordCloud(stopwords=set(STOPWORDS), background_color='white').generate(negative_word) #color_func=lambda *args, **kwargs: "red" -> to add color in words
plt.figure(figsize=(10,6))
plt.imshow(wordcloud_negative)
plt.axis('off')
plt.show()
#

wordcloud_positive = WordCloud(stopwords=set(STOPWORDS), background_color='white').generate(positive_word) #color_func=lambda *args, **kwargs: "red" -> to add color in words
plt.figure(figsize=(10,6))
plt.imshow(wordcloud_positive)
plt.axis('off')
plt.show()
#

"""**Performing emoji analysis**"""

!pip install emoji==2.2.0

import emoji

top_emojis = []
for comment in a['comment_text']:
    for i in comment:
        if i in emoji.EMOJI_DATA:
            top_emojis.append(i)

top_emojis[0:11]

from collections import Counter

top_emojis_count = Counter(top_emojis).most_common(10)
print(top_emojis_count)
#

emoji = [Counter(top_emojis).most_common(10)[i][0] for i in range(10)]
frequency = [Counter(top_emojis).most_common(10)[i][1] for i in range(10)]
print(emoji)
print(frequency)
#

import plotly.express as px
df = pd.DataFrame({'emoji': emoji, 'frequency': frequency})

# Create the bar chart
fig = px.bar(df, x='emoji', y='frequency')  # Removed color argument as 'total_videos' isn't defined
fig.show()
#

"""**Understanding viewer reactions to different YouTube content**"""

# to check all the unique video_id
video = []
for i in a['video_id']:
  if i not in video:
    video.append(i)
print(video)
len(video)

# Efficiently create a Series with video IDs and sentiment counts
unique_video_counts = a.groupby('video_id')['sentiment'].value_counts().unstack(fill_value=0)

# Add a row for total sentiment count
unique_video_counts['total_comments'] = unique_video_counts.sum(axis=1)

print(unique_video_counts)  # Display the first few rows

# Find the video with the maximum count for each sentiment
max_positive_video = unique_video_counts['positive'].idxmax()
max_negative_video = unique_video_counts['negative'].idxmax()
max_neutral_video = unique_video_counts['neutral'].idxmax()

print("Video with maximum positive sentiment:", max_positive_video)
print("Video with maximum negative sentiment:", max_negative_video)
print("Video with maximum neutral sentiment:", max_neutral_video)

# Find the video with the maximum count for each sentiment
min_positive_video = unique_video_counts['positive'].idxmin()
min_negative_video = unique_video_counts['negative'].idxmin()
min_neutral_video = unique_video_counts['neutral'].idxmin()

print("Video with minimum positive sentiment:", min_positive_video)
print("Video with minimum negative sentiment:", min_negative_video)
print("Video with minimum neutral sentiment:", min_neutral_video)

"""**Analyzing if there is a correlation between comment length and sentiment**"""

# Analyze comment length
a['comment_length'] = a['comment_text'].str.len()

# Analyze correlation between polarity and comment length
correlation = a[['polarity', 'comment_length']].corr()
polarity_comment_length_correlation = correlation.loc['polarity', 'comment_length']

# Print correlation coefficient
print("Correlation between polarity and comment length:", polarity_comment_length_correlation)
# Create a scatter plot
sns.scatterplot(x='comment_length', y='polarity', data=a)
plt.xlabel('Comment Length')
plt.ylabel('Polarity')
plt.title('Correlation Between Comment Length and Polarity')
plt.show()

max_comment_length = a['comment_length'].max()
print(f"Maximum comment length: {max_comment_length} characters")
# Filter the DataFrame to get the row with the maximum comment length
longest_comment = a[a['comment_length'] == max_comment_length]

# Print the comment text and other details of the row with the maximum comment length
print("Longest comment:")
for index, row in longest_comment.iterrows():
    print("Comment Text:", row['comment_text'])
    print("Video ID:", row['video_id'])
    print("Likes:", row['likes'])
    print("Replies:", row['replies'])
    print("Polarity:", row['polarity'])
    print("Sentiment:", row['sentiment'])
    print()  # Add an empty line for better readability

"""**Analyzing which sentiment of comments(positive or negative or neutral) got more like**"""

# Ensure data types are correct
a['likes'] = pd.to_numeric(a['likes'], errors='coerce')
a['polarity'] = pd.to_numeric(a['polarity'], errors='coerce')

# Handle potential outliers or missing values (if necessary)
a = a[(a['likes'] > 0) & (a['polarity'].between(-1, 1))]

correlation = a[['polarity', 'likes']].corr()
polarity_likes_correlation = correlation.loc['polarity', 'likes']

# Print correlation coefficient
print("Correlation between polarity and likes:", polarity_likes_correlation)

# Create the scatter plot
sns.scatterplot(x='likes', y='polarity', data=a)
plt.xlabel('Likes')
plt.ylabel('Polarities')
plt.title('Relation Between Likes and Polarities')
plt.show()

path1 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/CAvideos.csv')
aa = pd.read_csv(path1, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(aa)
path2 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/DEvideos.csv')
bb = pd.read_csv(path2, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(bb)
path3 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/FRvideos.csv')
cc = pd.read_csv(path3, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(cc)
path4 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/GBvideos.csv')
dd = pd.read_csv(path4, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(dd)
path5 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/INvideos.csv')
ee = pd.read_csv(path5, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(ee)
path6 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/JPvideos.csv')
ff = pd.read_csv(path6, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(ff)
path7 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/KRvideos.csv')
gg = pd.read_csv(path7, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(gg)
path8 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/MXvideos.csv')
hh = pd.read_csv(path8, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(hh)
path9 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/RUvideos.csv')
ii = pd.read_csv(path9, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(ii)
path10 = (r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/USvideos.csv')
jj = pd.read_csv(path10, on_bad_lines='skip', encoding = 'iso-8859-1')
#print(jj)

total_countries_data = pd.concat([aa, bb, cc, dd, ee, ff, gg, hh, ii, jj], axis = 0)
print(total_countries_data)

total_countries_data.shape

total_countries_data.info()

# to check the missing values
# Assuming 'total_countries_data' is your DataFrame
missing_indices = total_countries_data.index.difference(range(len(total_countries_data)))

if len(missing_indices) == 0:
  print("There are no missing indices in your DataFrame.")
else:
  print("There are missing indices. You might not be seeing the entire DataFrame.")

total_countries_data = total_countries_data.reset_index(drop=True) #to reset index values
print(total_countries_data)

total_countries_data.isnull().sum()

total_countries_data.duplicated()

total_countries_data.duplicated().sum()

total_countries_data.drop_duplicates(inplace=True) #dropping duplicates
total_countries_data.shape
#

total_countries_data.to_csv(r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data\total_countries_data.csv', index=False) #to save file in csv format
#

from sqlalchemy import create_engine

# Correct format for connecting to a SQLite file
engine = create_engine('sqlite://///content/drive/MyDrive/Python Project Practise/Youtube/youtube_sample.sqlite')
# Use 'sqlite:///' followed by the absolute file path to the SQLite database

total_countries_data.to_sql('Users', con=engine, if_exists='replace', index=False) #to save file in sqlite format
#

"""**Analyzing most liked category**"""

total_countries_data['category_id'].unique()

json_us_categories = pd.read_json(r'/content/drive/MyDrive/Python Project Practise/Youtube/additional data/US_category_id.json')
json_us_categories

json_us_categories['items'][0]

#cate = []
#for i in json_us_categories['items']:
  #cate.append(i['snippet']['title'])
#print(cate)

cate = {}
for item in json_us_categories['items'].values:
  cate[int(item['id'])] = (item['snippet']['title'])
print(cate)

total_countries_data['category_name'] = total_countries_data['category_id'].map(cate)
total_countries_data

plt.figure(figsize=(15,8))
sns.boxplot(x= 'category_name', y= 'likes', data=total_countries_data)
plt.xticks(rotation=90)
plt.show()
# most liked category
#

"""**Analyzing whether the audiance is engaged or not**"""

total_countries_data['likes_rate'] = (total_countries_data['likes']/total_countries_data['views'])*100
total_countries_data['dislikes_rate'] = (total_countries_data['dislikes']/total_countries_data['views'])*100
total_countries_data['comments_count_rate'] = (total_countries_data['comment_count']/total_countries_data['views'])*100
total_countries_data
#

mostliked = total_countries_data.sort_values('likes_rate', ascending=False).groupby('category_name')['likes_rate'].max()
#
print(mostliked)

plt.figure(figsize=(15,8))
sns.boxplot(x= 'category_name', y= 'likes_rate', data=total_countries_data)
plt.xticks(rotation=90)
plt.show()
#

mostdisliked = total_countries_data.sort_values('dislikes_rate', ascending=False).groupby('category_name')['dislikes_rate'].max()
#
print(mostdisliked)

plt.figure(figsize=(15,8))
sns.boxplot(x= 'category_name', y= 'dislikes_rate', data=total_countries_data)
plt.xticks(rotation=90)
plt.show()
#

most_comments_count_rate = total_countries_data.sort_values('comments_count_rate', ascending=False).groupby('category_name')['comments_count_rate'].max()
#
print(most_comments_count_rate)

plt.figure(figsize=(15,8))
sns.boxplot(x= 'category_name', y= 'comments_count_rate', data=total_countries_data)
plt.xticks(rotation=90)
plt.show()
#

plt.figure(figsize=(15,8))
sns.regplot(x= 'views', y= 'likes', data=total_countries_data)
#

zxc = total_countries_data[['views', 'likes', 'dislikes']].corr()
print(zxc)
#

"""if my views increase by 100 or 1% then my likes will increase by 78 or 78%"""

sns.heatmap(total_countries_data[['views', 'likes', 'dislikes']].corr(), annot=True)
#

"""**Analyzing trending videos of youtube**"""

cdf = total_countries_data.groupby(['channel_title']).size().sort_values(ascending = False).reset_index()
#
cdf = cdf.rename(columns={0:'total_videos'})
cdf
#

# total_countries_data['channel_title'].value_counts()

import plotly.express as px
px.bar(cdf.head(20), x='channel_title', y='total_videos', color='total_videos')
#

"""**Analyzing does punctuations in title have impact on views, likes and dislikes**"""

import string

string.punctuation

[i for i in total_countries_data['title'][0] if i in string.punctuation]

def punc_count(text):
  return len([i for i in text if i in string.punctuation])

total_countries_data['punc_count'] = total_countries_data['title'].apply(punc_count)
total_countries_data
#

plt.figure(figsize=(15,8))
sns.boxplot(x= 'punc_count', y= 'views', data=total_countries_data)
plt.show()
#

plt.figure(figsize=(15,8))
sns.boxplot(x= 'punc_count', y= 'likes', data=total_countries_data)
plt.show()
#

"""**Most common words used in titles including pounctuations**"""

!pip install nltk

import nltk
from collections import Counter
nltk.download('stopwords') # Download the stopwords data
from nltk.corpus import stopwords

# Preprocess text
total_countries_data['title_lower'] = total_countries_data['title'].str.lower()
stop_words = set(stopwords.words('english'))
total_countries_data['title_words'] = total_countries_data['title_lower'].apply(lambda x: [word for word in x.split() if word not in stop_words])

# Flatten the list of words
all_words = [word for sublist in total_countries_data['title_words'] for word in sublist]

# Create word frequency distribution
word_counts = Counter(all_words)

# Get most common words
most_common_words = word_counts.most_common(20)

# Visualization
plt.bar([word for word, count in most_common_words], [count for word, count in most_common_words])
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Most Common Words in Video Titles (Including Punctuation)')
plt.xticks(rotation=45)
plt.show()

import nltk
from collections import Counter
nltk.download('stopwords') # Download the stopwords data
from nltk.corpus import stopwords
import string

# Preprocess text
total_countries_data['title_lower'] = total_countries_data['title'].str.lower()

# Define punctuation to remove
punctuations = set(string.punctuation)

stop_words = set(stopwords.words('english'))

def remove_punctuation_and_stopwords(text):
  """
  Removes punctuation and stop words from a given text string.
  """
  words = [word for word in text.split() if word not in stop_words and word not in punctuations]
  return words

total_countries_data['title_words'] = total_countries_data['title_lower'].apply(remove_punctuation_and_stopwords)

# Flatten the list of words
all_words = [word for sublist in total_countries_data['title_words'] for word in sublist]

# Create word frequency distribution
word_counts = Counter(all_words)

# Get most common words
most_common_words = word_counts.most_common(20)

# Visualization
plt.bar([word for word, count in most_common_words], [count for word, count in most_common_words])
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Most Common Words in Video Titles (Excluding Punctuation)')
plt.xticks(rotation=45)
plt.show()

"""**For a video, analyzing the number of views, likes, dislikes, and comments changes over time**"""

total_countries_data['video_id'].unique().shape

total_countries_data['trending_date'] = pd.to_datetime(total_countries_data['trending_date'], format='%y.%d.%m')
# Convert publish_time to datetime if not already
total_countries_data

total_countries_data['publish_time'] = pd.to_datetime(total_countries_data['publish_time'])
total_countries_data

video_data = total_countries_data[total_countries_data['video_id'] == 'n1WpP7iowLc']
video_data
#

# Convert trending_date to datetime index
video_data.set_index('trending_date', inplace=True)

# Plotting
video_data['views'].plot(figsize=(12, 6), title='Daily Views for Video n1WpP7iowLc')
plt.ylabel('Views')
plt.show()

# Assuming video_data is your DataFrame with a datetime index

# Line chart for views, likes, and dislikes
video_data[['views', 'likes', 'dislikes']].plot(figsize=(12, 6))
plt.title('Views, Likes, and Dislikes Over Time')
plt.ylabel('Count')
plt.show()

# Scatter plot for views vs. likes
video_data.plot.scatter(x='views', y='likes', figsize=(8, 6))
plt.title('Views vs. Likes')
plt.show()